{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4 COVID-19 Prediction with Interpret_ML\n",
    "This notebook will describe attempt at predicting the amount of Confirmed and Fatalities for the 3rd week of the COVID-19 Kaggle Competition, using models created from the [Interpret_ML toolbox](https://github.com/interpretml/interpret)\n",
    "\n",
    "## Data Sources & Collection\n",
    "We're using data that was collected or scraped from various sources, some of which are courtesy of work already done by other people that will be credited. Other data that we're presenting (and will be appending to the training data) are collected from multiple other sources, using some tools as can be seen in the Github page [here](). The list of sources as well as the sources that we'll be featuring in this notebook are listed here, namely:\n",
    "\n",
    "1. [Worldometer Coronavirus page](https://www.worldometers.info/coronavirus/), which we believe contains the most updated information on the number of Confirmed and Fatalities that happen globally. As of 5 April, noted to have been updated to contain the latest amount of tests that happen globally, however noted that no time series for all countries are provided yet (in Worldometer itself).\n",
    "2. Global climate Data from [Worldbank](https://datahelpdesk.worldbank.org/knowledgebase/articles/902061-climate-data-api). As explained a bit later in the notebook, we believe that a country's current climate condition might have a bit of effect on the spread of the virus.\n",
    "3. [Our World in Data](https://ourworldindata.org/covid-testing), who has provided quite an updated time series for the recorded tests conducted by many countries for COVID-19. It is to be noted however, due to not all countries having released test data, only several countries could have their data imputed (and not by region)\n",
    "4. [The COVID Tracking Project](https://covidtracking.com/ ), to specifically provide data COVID-19 testing that has so far been recorded in the US. It is noted and understood that this will only be helping mainly to predict the outcome in US and its region\n",
    "\n",
    "## Short Introduction to InterpretML\n",
    "[InterpretML](https://github.com/interpretml/interpret) is a Machine Learning toolbox developed by Microsoft Research, with the goal of giving better interpretability to trained Machine Learning models. For COVID-19 forecasting in particular, we believe that this toolbox will provide better understanding of the correlation between many different features and the model's prediction, hopefully helping in answering some of the [scientific questions](https://www.kaggle.com/c/covid19-global-forecasting-week-4/overview/open-scientific-questions) regardng the factors which effect COVID-19 transmission.\n",
    "\n",
    "[TODO: summarize what InterpretML is, and provide some of the model examples that can be used from the InterpretML toolbox]\n",
    "\n",
    "For this notebook, we'll create several models from the [InterpretML toolbox library](https://github.com/interpretml/interpret). These models will be trained using different sets of features (including the default features provided), which will then have their performances be compared to each other.  \n",
    "\n",
    "## Loading of Interpret_ML.\n",
    "First ensure that the Interpret_ML toolbox is installed with pip   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "!pip install -U interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from interpret import show\n",
    "from interpret.data import Marginal\n",
    "from interpret.glassbox import ExplainableBoostingRegressor, LinearRegression, RegressionTree\n",
    "from interpret.perf import RegressionPerf\n",
    "\n",
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Appending of the the Training Dataset with other Features\n",
    "Now that Interpret_ML has been installed, let's first review and take note of the training and test data that has been provided by default, to see what features could be extracted for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "train_default_path = \"../input/train.csv\"\n",
    "test_default_path = \"../input/test.csv\"\n",
    "\n",
    "train_default_data = pd.read_csv(train_default_path)\n",
    "#train_default_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_default_data = pd.read_csv(test_default_path)\n",
    "#test_default_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at these data, it can be seen that the number of previously known number of Confirmed and Fatalities would be the main default features that could be extracted and used. Based on expert opinions as well as various other works however, it seems that these features would not be sufficient in accurately predicting the total amount of Confirmed and Fatalities in the future.\n",
    "\n",
    "Hence, additional data features would be required. In this notebook, several of the additional data features that we've collected can be seen below:\n",
    "\n",
    "### 1.a. Weather features\n",
    "Thanks to the work by David Bonin (Kaggle user [davidbn92](https://www.kaggle.com/davidbnn92)) in his [notebook](https://www.kaggle.com/davidbnn92/weather-data/output), a variation of the training data that has been appended with Weather/climate features of all regions has been provided. As noted in their page, these weather data are courtesy of [NOAA GSOD readings](https://www.kaggle.com/noaa/gsod), which has been appended to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_appended_df = pd.read_csv(\"../input/training_data_with_weather_info_week_4.csv\")\n",
    "print(\"Current columns:\", train_appended_df.columns)\n",
    "#train_appended_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_unique_regions = train_appended_df['Province_State'].unique()\n",
    "#training_data_unique_regions, len(training_data_unique_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As per noted by David in his work, the weather features that were added included the following:\n",
    "\n",
    "- ```temp```: Mean temperature for the day in degrees Fahrenheit to tenths.\n",
    "- ```max```: Maximum temperature reported during the day in Fahrenheit to tenths--time of max temp report varies by country and region, so this will sometimes not be the max for the calendar day.\n",
    "- ```min```: Minimum temperature reported during the day in Fahrenheit to tenths--time of min temp report varies by country and region, so this will sometimes not be the min for the calendar day.\n",
    "- ```stp```: Mean station pressure for the day in millibars to tenths.\n",
    "- ```slp```: Mean sea level pressure for the day in millibars to tenths.\n",
    "- ```dewp```: Mean dew point for the day in degrees Fahrenheit to tenths.\n",
    "- ```wdsp```: Mean wind speed for the day in knots to tenths.\n",
    "- ```prcp```: Total precipitation (rain and/or melted snow) reported during the day in inches and hundredths; will usually not end with the midnight observation--i.e., may include latter part of previous day. .00 indicates no measurable precipitation (includes a trace).\n",
    "- ```fog```: Indicators (1 = yes, 0 = no/not reported) for the occurrence during the day\n",
    "\n",
    "The reason to include weather data for COVID-19 prediction would is because of some previous research (example of such paper [here](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2916580/)) linking the coronavirus family having [seasonality period](https://www.bbc.com/future/article/20200323-coronavirus-will-hot-weather-kill-covid-19), with indication that warmer weather could [slow down](https://www.theguardian.com/world/2020/apr/05/scientists-ask-could-summer-heat-help-beat-covid-19) the transmission of the virus. However, similarly there has been caution by health experts that this might not be [true](https://www.sciencenews.org/article/coronavirus-warm-weather-will-not-slow-covid-19-transmission). \n",
    "\n",
    "As such, we'll investigate using the InterpretML toolbox to see the correlation between any of these weather effect with COVID-19 forecasting.\n",
    "\n",
    "### 1.b. Population Data \n",
    "Specifically, the Population Density for each region. Hypothetically, a region that has a higher population density should in theory have a higher chance of faster COVID-19 transmission. For consistency, we'll be mainly using the countries' and regions' population and population density data that was recorded by [Worldometer](https://www.worldometers.info/world-population/population-by-country/) from their respective country pages. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_df = pd.read_csv(\"../input/Worldometer_Population_Regional_Latest.csv\")\n",
    "#population_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list the unique regions in the population_df DataFrame, while also removing the 'All_Regions' tag (which indicate it's the population of the whole country, and not just a region)\n",
    "popdf_unique_regions = population_df['Region'].unique()\n",
    "popdf_unique_regions = np.sort(popdf_unique_regions[popdf_unique_regions != 'All_Regions'])\n",
    "print(\"All {} unique regions recorded:\".format(str(len(popdf_unique_regions))))\n",
    "print(popdf_unique_regions, \"True Victoria\" in popdf_unique_regions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note that 'All_regions' mean that the data shown in that particular row applies to the whole country, not just a particular region in that country)\n",
    "\n",
    "However, as can be seen it is noted that Worldometer doesn't seem to provide the Population Density features recorded for regional levels. Hence, for countries with regions in the training data, we'll instead use the countries' and regions' population data as of 2019 provided by OECD on their [Region and Cities](https://stats.oecd.org/Index.aspx?DataSetCode=REGION_DEMOGR#) page. It is noted that these would not likley reflect the lates population density for all region/provinces in the training data. However, we believe the difference in population density for these regions between 2019 and 2020 should be minimal enough such that the the difference should be rather minimal. A reliable source that could help with this would be helpful as an input/feedback.\n",
    "\n",
    "Note that OECD divides the regions into 2 types: T2 (Large) and T3 (Small) Regions. Let's take a glimpse at the population density records for all region types in 2019 first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population_density_area_df = pd.read_csv(\"../input/OECD_PopulationDensity_and_Area-T2_T3_Regions-2018_2019.csv\")\n",
    "print(\"Columns available:\", population_density_area_df.columns)\n",
    "population_density_area_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limit to only population density data, and in Year 2019 only\n",
    "population_density_only = population_density_area_df[population_density_area_df[\"VAR\"] == \"POP_DEN\"]\n",
    "population_density_only.drop(['SEX', 'Gender', 'POS', 'Position', 'PowerCode Code', 'Reference Period Code', 'Reference Period'], axis=1)\n",
    "population_density_2019 = population_density_only[population_density_only[\"Year\"] == 2019]\n",
    "population_density_unique_regions = population_density_2019['Region'].unique()\n",
    "print(\"All unique {} regions recorded for OECD's population density data: \".format(str(len(population_density_unique_regions))), \n",
    "                                                                                    population_density_unique_regions)\n",
    "population_density_2019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding these population data into the modified training_data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "#train_appended_df = train_appended_df.copy()\n",
    "#Initiate new feature columns\n",
    "added_features = ['Population (2020)', 'Population Density']\n",
    "for feature in added_features:\n",
    "    train_appended_df[feature] = 0\n",
    "\n",
    "for country in train_appended_df['Country_Region'].unique():\n",
    "    #print(train_appended_df['Population (2020)'].unique())\n",
    "    country_segment = train_appended_df[train_appended_df['Country_Region'] == country]\n",
    "\n",
    "    #Sanity check for several countries, as they're apparently named quite differently in Worldometers vs the training data\n",
    "    if country == \"Burma\":\n",
    "        country = \"Myanmar\" #Burma in Training data is actually Myanmar. History stuff I guess?\n",
    "    elif country == \"Korea, South\":\n",
    "        country = \"South Korea\" #this one is honestly just trolling at this point...\n",
    "\n",
    "    population_df_country = population_df[population_df['Country (or dependency)'] == country]#['Region'] == 'All_Regions'\n",
    "\n",
    "    #check whehter the current country has any listed states/regions in the original training data.\n",
    "    #If yes: Add regional population and regional population density data\n",
    "    #If not: Only add country population and population density data.\n",
    "    country_regions_training = list(country_segment['Province_State'].unique())\n",
    "\n",
    "    #Apparently there are 2 'Congo'-s: Republic of Congo/Brazzaville, vs DEMOCRATIC Republic of Congo/Zaire (as how it's differentiated in Worldometers)\n",
    "    if country == \"Congo (Brazzaville)\": \n",
    "        country = \"Congo\"\n",
    "        country_regions_training = [\"Brazzaville\"]\n",
    "    elif country == \"Congo (Kinshasa)\":\n",
    "        country = \"Congo\"\n",
    "        country_regions_training = [\"Kinshasa\"]\n",
    "\n",
    "    if country_regions_training == [np.NaN]:\n",
    "        #print(country, country_regions_training)\n",
    "        \n",
    "        #sanity check: in case country isn't listed in the worldometers population data, \n",
    "        #then query to the population_df would return DataFrame of 0\n",
    "        if len(population_df_country) != 0:\n",
    "            try:\n",
    "                country_population = int(population_df_country[population_df_country['Region'] == \"All_Regions\"][\"Population (2020)\"].values[0].replace(\",\", \"\")) \n",
    "            except:\n",
    "                print(\"Problematic country for pop. df\", country, country_regions_training)\n",
    "            try:\n",
    "                country_population_density = population_df_country[population_df_country['Region'] == \"All_Regions\"][\"Density (P/KmÂ²)\"].values[0]\n",
    "            except:\n",
    "                print(\"Problematic country for pop_density df\", country, country_regions_training)\n",
    "                break\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        country_ids = country_segment.index.tolist()\n",
    "        train_appended_df.loc[country_ids, ['Population (2020)']] = country_population\n",
    "        train_appended_df.loc[country_ids, ['Population Density']] = country_population_density\n",
    "    else:\n",
    "        for region in country_regions_training:\n",
    "            region_segment = country_segment[country_segment['Province_State'] == region]\n",
    "\n",
    "            #sanity check, as apparently the region names are not truly unique to a country in Worldometer's data\n",
    "            #(in particular, the region 'Victoria' which is unique to Australia in training data, is not present in Worldometer's Australia,\n",
    "            # and instead available for other countries.)\n",
    "            region_popdf_segment = population_df_country[population_df_country['Region'] == region]\n",
    "            region_popdensity_segment = population_density_2019[population_density_2019['Region'] == region]\n",
    "\n",
    "            if len(region_popdf_segment) == 1: #Means that there is a valid row available in Worldometer's population_data\n",
    "                region_population = int(region_popdf_segment[\"Population (2020)\"].values[0].replace(\",\", \"\") )\n",
    "                #region_population = int(population_df_country[population_df_country['Region'] == region][\"Population (2020)\"].values[0].replace(\",\", \"\"))\n",
    "            else:\n",
    "                region_population = np.NaN\n",
    "\n",
    "            if len(region_popdensity_segment) == 1:\n",
    "                region_population_density = population_density_2019[population_density_2019['Region'] == region]['Value'].values[0]\n",
    "            else:\n",
    "                region_population_density = np.NaN\n",
    "\n",
    "            region_ids = region_segment.index.tolist()\n",
    "            train_appended_df.loc[region_ids, ['Population (2020)']] = region_population\n",
    "            train_appended_df.loc[region_ids,['Population Density']] = region_population_density\n",
    "train_appended_df.head()\n",
    "#print(\"Done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deal with missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some missing values in some of the appended features. <br> Since interpretml doesn't support missing features, we need to fill these missing values when we can. <br>\n",
    "\n",
    "Weather:\n",
    "* Fill in max, min with temp\n",
    "* Too many missing values for slp,dewp,rh,ah, so we won't use it\n",
    "\n",
    "Population: \n",
    "* Population (2020): <br>\n",
    "Missing values caused because we don't have specific population for country+province, hence we will use data of country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dummy = \"max\"\n",
    "missing_index = np.where(train_appended_df[feature_dummy].isnull())[0]\n",
    "train_appended_df.at[missing_index,feature_dummy] = train_appended_df[\"temp\"][missing_index]\n",
    "\n",
    "feature_dummy = \"min\"\n",
    "missing_index = np.where(train_appended_df[feature_dummy].isnull())[0]\n",
    "train_appended_df.at[missing_index,feature_dummy] = train_appended_df[\"temp\"][missing_index]\n",
    "\n",
    "\"\"\"\n",
    "feature_dummy = \"Population Density\"\n",
    "missing_index = np.where(train_appended_df[feature_dummy].isnull())[0]\n",
    "missing_country_province = train_appended_df[\"country+province\"][missing_index].unique()\n",
    "for reg in missing_country_province:\n",
    "    mask_ = train_appended_df[\"country+province\"] == reg\n",
    "    mask_ix_ = np.where(mask_)[0]\n",
    "    mask_country_ = train_appended_df[\"country+province\"] == reg.split('')\n",
    "    if(len(np.where())):\n",
    "        replacement = \n",
    "    else:\n",
    "        print(\"No replacement found for {}\".format(reg))\n",
    "    train_appended_df[feature_dummy].at[mask_ix_,feature_dummy] = \n",
    "\"\"\"\n",
    "#np.where(train_appended_df[\"country+province\"] ==\"Australia\")\n",
    "\n",
    "#feature_dummy = \"Population (2020)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(train_appended_df[\"Population (2020)\"].isnull()))\n",
    "np.sum(train_appended_df[\"Population Density\"].isnull())\n",
    "np.sum(train_appended_df[\"max\"].isnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observing which feature is significant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df,\n",
    "               features = ['Days','Region',\"prev_ConfirmedCases\",\"prev_Fatalities\"],\n",
    "               targets = [\"ConfirmedCases\", \"Fatalities\"]):\n",
    "\n",
    "    # Create category called Region: country_province\n",
    "    region_list = [\"{}_{}\".format(df[\"Country_Region\"][i], df[\"Province_State\"][i]) for i in range(df.shape[0])]\n",
    "    df[\"Region\"]=region_list\n",
    "\n",
    "    # Get first day of corona virus for each region\n",
    "    unique_region_list = list(set(region_list))\n",
    "    unique_region_list.sort()\n",
    "    first_date_dict = {}\n",
    "    for region in unique_region_list:\n",
    "        mask = df[\"Region\"]==region\n",
    "        first_ix = np.where(df[mask][\"ConfirmedCases\"]>0)[0][0] -1    \n",
    "        first_date = df[mask][\"Date\"].iloc[first_ix]\n",
    "        first_date_dict[region] = first_date\n",
    "\n",
    "    # add column \"Days\": number of days since the first day of case per each region\n",
    "    def get_days(dt):\n",
    "        return dt.days\n",
    "    dummy = [first_date_dict[region] for region in df[\"Region\"]]\n",
    "    df[\"Days\"]=(pd.to_datetime(df['Date'])-pd.to_datetime(dummy)).apply(get_days)\n",
    "\n",
    "    # Add previous confirmed cases and previous fatalities to df\n",
    "    loc_group=[\"Region\"]\n",
    "    for target in targets:\n",
    "        df[\"prev_{}\".format(target)] = df.groupby(loc_group)[target].shift()\n",
    "        df[\"prev_{}\".format(target)].fillna(0, inplace=True)\n",
    "    \n",
    "    for target in targets:\n",
    "        df[target] = np.log1p(df[target])\n",
    "        df[\"prev_{}\".format(target)] = np.log1p(df[\"prev_{}\".format(target)])\n",
    "    \n",
    "    X = df[features]\n",
    "    Y = df[targets]\n",
    "    \n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Region',\"prev_ConfirmedCases\",\"prev_Fatalities\",\n",
    "            'Days',\"day_from_jan_first\",\n",
    "            \"temp\",\"max\",\"min\",\"prcp\",\"stp\",\"prcp\",\"fog\",\"wdsp\", # weather\n",
    "            \"Lat\",\"Long\"]\n",
    "X,Y = preprocess(train_appended_df,features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "marginal = Marginal().explain_data(X, Y[\"ConfirmedCases\"],\"ConfirmedCases\")\n",
    "show(marginal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observing the data above, we can conclude that the weather data is not significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Predict with Explainable Boosting Machine (EBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Region',\"prev_ConfirmedCases\",\"prev_Fatalities\",\n",
    "            'Days',\n",
    "            \"Lat\",\"Long\"]\n",
    "X,Y = preprocess(train_appended_df,features=features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have a prediction of the accuracy of our prediction model, we split the original data into train and validation. We use the train for training, and use the model to predict on the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(X,Y, unique_region_list,num_of_val_days):\n",
    "    \n",
    "    train_ix = []\n",
    "    val_ix = []\n",
    "    for region in unique_region_list:\n",
    "        \n",
    "        mask = X[\"Region\"]==region\n",
    "        ix = np.where(mask)[0]\n",
    "        \n",
    "        train_ix += list(ix[:-num_of_val_days].flatten())\n",
    "        val_ix += list(ix[-num_of_val_days:].flatten())\n",
    "        \n",
    "    return X.iloc[train_ix],X.iloc[val_ix],Y.iloc[train_ix],Y.iloc[val_ix]    \n",
    "\n",
    "# IMPORTANT NOTE: We can only use prev_ConfirmedCases for the first day to predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT NOTE: assuming that X_features is sorted by number of days \"Days\"\n",
    "ENFORCE_CONSTRAINT = True\n",
    "seed = 1\n",
    "def evaluate_rmse(Y_predicted,Y_true):\n",
    "    \"\"\"\n",
    "    Y_predicted: n-by-d n is the number of data points, d is the number of criteria\n",
    "    Y_true: n-by-d\n",
    "    OUTPUT\n",
    "    d elements\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_error(Y_predicted,Y_true,multioutput='raw_values'))\n",
    "\n",
    "def predict(X_features,Y,num_validation_days,num_days_to_predict):\n",
    "    unique_region_list = list(set(X_features[\"Region\"]))\n",
    "    unique_region_list.sort()\n",
    "    print(\"No of unique region list: {}\".format(len(unique_region_list)))\n",
    "    \n",
    "    ##################################################################\n",
    "    # Train and Validation\n",
    "    ##################################################################\n",
    "    # Split to train and validation\n",
    "    X_train,X_val,Y_train,Y_val = split_train_val(X,Y, unique_region_list,num_validation_days)\n",
    "    \n",
    "    # Train\n",
    "    model_ConfirmedCases = ExplainableBoostingRegressor(random_state=seed)\n",
    "    model_ConfirmedCases.fit(X_train,Y_train[\"ConfirmedCases\"])\n",
    "    model_Fatalities = ExplainableBoostingRegressor(random_state=seed)\n",
    "    model_Fatalities.fit(X_train,Y_train[\"Fatalities\"])\n",
    "    \n",
    "    # Predict for val\n",
    "    Y_val_predicted = np.zeros((X_val.shape[0],2))\n",
    "    \n",
    "    for i in range(X_val.shape[0]):\n",
    "        \n",
    "        if(i==0 or X_val.iloc[i-1][\"Region\"] != X_val.iloc[i][\"Region\"]):\n",
    "            pred_ConfirmedCases = model_ConfirmedCases.predict(X_val.iloc[[i]])[0]\n",
    "            pred_Fatalities = model_Fatalities.predict(X_val.iloc[[i]])[0]\n",
    "            \n",
    "            if(ENFORCE_CONSTRAINT):\n",
    "                if(pred_ConfirmedCases<X_val.iloc[[i]][\"prev_ConfirmedCases\"].item()):\n",
    "                    pred_ConfirmedCases = 1.*X_val.iloc[[i]][\"prev_ConfirmedCases\"].item()\n",
    "                if(pred_Fatalities<X_val.iloc[[i]][\"prev_Fatalities\"].item()):\n",
    "                    pred_Fatalities = X_val.iloc[[i]][\"prev_Fatalities\"].item()\n",
    "                    \n",
    "        else:\n",
    "            X_dummy  = X_val.iloc[[i]].copy(deep=True)\n",
    "            X_dummy[\"prev_ConfirmedCases\"] = pred_ConfirmedCases\n",
    "            X_dummy[\"prev_Fatalities\"] = pred_Fatalities\n",
    "            pred_ConfirmedCases = model_ConfirmedCases.predict(X_dummy)\n",
    "            pred_Fatalities =model_Fatalities.predict(X_dummy)\n",
    "        \n",
    "            if(ENFORCE_CONSTRAINT):\n",
    "                if(pred_ConfirmedCases<X_dummy[\"prev_ConfirmedCases\"].item()):\n",
    "                    pred_ConfirmedCases = 1.* X_dummy[\"prev_ConfirmedCases\"].item()\n",
    "                if(pred_Fatalities<X_dummy[\"prev_Fatalities\"].item()):\n",
    "                    pred_Fatalities = X_dummy[\"prev_Fatalities\"].item()\n",
    "                    \n",
    "        Y_val_predicted[i,0] = pred_ConfirmedCases\n",
    "        Y_val_predicted[i,1] = pred_Fatalities\n",
    "        \n",
    "    # Report validation accuracy\n",
    "    val_rmse = evaluate_rmse(Y_val,Y_val_predicted)\n",
    "    \n",
    "    ##################################################################\n",
    "    # Train w Full Model and Predict for Test\n",
    "    ##################################################################\n",
    "    # Train with full data\n",
    "    model_full_ConfirmedCases = ExplainableBoostingRegressor(random_state=seed)\n",
    "    model_full_ConfirmedCases.fit(X_features,Y[\"ConfirmedCases\"])\n",
    "    model_full_Fatalities = ExplainableBoostingRegressor(random_state=seed)\n",
    "    model_full_Fatalities.fit(X_features,Y[\"Fatalities\"])\n",
    "    \n",
    "    # Predict for test\n",
    "    Y_test_predicted = np.zeros((len(unique_region_list)*num_days_to_predict,2))\n",
    "    count=0\n",
    "    for region in unique_region_list:\n",
    "        mask = X_features[\"Region\"]==region\n",
    "        \n",
    "        prev_ConfirmedCase_ = Y[mask][\"ConfirmedCases\"].iloc[-1]\n",
    "        prev_Fatality_ = Y[mask][\"Fatalities\"].iloc[-1]\n",
    "        \n",
    "        #print(prev_ConfirmedCase_,np.exp(prev_ConfirmedCase_)-1, prev_Fatality_, np.exp(prev_Fatality_)-1)\n",
    "        \n",
    "        X_dummy = X[mask].iloc[[-1]].copy(deep=True)\n",
    "        X_dummy[\"prev_ConfirmedCases\"] = prev_ConfirmedCase_\n",
    "        X_dummy[\"prev_Fatalities\"] = prev_Fatality_\n",
    "        X_dummy[\"Days\"] = X_dummy[\"Days\"]+1\n",
    "        \n",
    "        pred_ConfirmedCases = model_full_ConfirmedCases.predict(X_dummy)\n",
    "        pred_Fatalities = model_full_Fatalities.predict(X_dummy)\n",
    "        \n",
    "        if(ENFORCE_CONSTRAINT):\n",
    "            if(pred_ConfirmedCases<X_dummy[\"prev_ConfirmedCases\"].item()):\n",
    "                pred_ConfirmedCases = X_dummy[\"prev_ConfirmedCases\"].item()\n",
    "            if(pred_Fatalities<X_dummy[\"prev_Fatalities\"].item()):\n",
    "                pred_Fatalities = X_dummy[\"prev_Fatalities\"].item()\n",
    "                \n",
    "        Y_test_predicted[count,0] = pred_ConfirmedCases\n",
    "        Y_test_predicted[count,1] = pred_Fatalities\n",
    "        count = count+1\n",
    "        \n",
    "        for days_ahead in range(2,num_days_to_predict+1):\n",
    "            \n",
    "            X_dummy[\"prev_ConfirmedCases\"] = pred_ConfirmedCases\n",
    "            X_dummy[\"prev_Fatalities\"] = pred_Fatalities\n",
    "            X_dummy[\"Days\"] = X_dummy[\"Days\"]+1\n",
    "            pred_ConfirmedCases = model_full_ConfirmedCases.predict(X_dummy)\n",
    "            pred_Fatalities = model_full_Fatalities.predict(X_dummy)\n",
    "            \n",
    "            if(ENFORCE_CONSTRAINT):\n",
    "                if(pred_ConfirmedCases<X_dummy[\"prev_ConfirmedCases\"].item()):\n",
    "                    pred_ConfirmedCases = X_dummy[\"prev_ConfirmedCases\"].item()\n",
    "                if(pred_Fatalities<X_dummy[\"prev_Fatalities\"].item()):\n",
    "                    pred_Fatalities = X_dummy[\"prev_Fatalities\"].item()\n",
    "                \n",
    "            Y_test_predicted[count,0] = pred_ConfirmedCases\n",
    "            Y_test_predicted[count,1] = pred_Fatalities\n",
    "            \n",
    "            count = count+1\n",
    "      \n",
    "    assert count==len(Y_test_predicted), \"Something wrong\"\n",
    "    \n",
    "\n",
    "    return unique_region_list,X_val,Y_val,Y_val_predicted,val_rmse,Y_test_predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days_to_predict = 43\n",
    "num_validation_days = 10\n",
    "unique_region_list,X_val,Y_val,Y_val_predicted,val_rmse,Y_test_predicted=predict(X,Y,num_validation_days,num_days_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation error\n",
    "print(\"RMSE for ConfirmedCases and Fatalities: {}\".format(val_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the final value ConfirmedCases and Fatalities\n",
    "# Convert back to linear scale\n",
    "Y_test_predicted_final = np.exp(Y_test_predicted)-1\n",
    "Y_val_predicted_final = np.exp(Y_val_predicted)-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test_predicted_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose using region_ix\n",
    "#region_ix = 3\n",
    "#region = unique_region_list[region_ix]\n",
    "\n",
    "# Choose using region\n",
    "region = \"Indonesia_nan\"\n",
    "region_ix = unique_region_list.index(region)\n",
    "\n",
    "USE_LOG_SCALE=False\n",
    "PLOT_LINE = False\n",
    "##############################################\n",
    "\n",
    "mask = X[\"Region\"]==region\n",
    "N = Y[mask].shape[0]\n",
    "x_ = np.arange(N+num_days_to_predict)\n",
    "\n",
    "validation_confirmed_cases = Y_val_predicted_final[region_ix*num_validation_days:(region_ix+1)*num_validation_days,0]\n",
    "validation_fatalities = Y_val_predicted_final[region_ix*num_validation_days:(region_ix+1)*num_validation_days,1]\n",
    "predicted_confirmed_cases = Y_test_predicted_final[region_ix*num_days_to_predict:(region_ix+1)*num_days_to_predict,0]\n",
    "predicted_fatalities = Y_test_predicted_final[region_ix*num_days_to_predict:(region_ix+1)*num_days_to_predict,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 8\n",
    "gs  = gridspec.GridSpec(1, 2, width_ratios=[1, 1],wspace=0.25)\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(gs[0,0])\n",
    "ax2 = fig.add_subplot(gs[0,1])\n",
    "if(USE_LOG_SCALE):\n",
    "    ax1.set_yscale('log')\n",
    "    ax2.set_yscale('log')\n",
    "ax1.scatter(x_[:N],np.exp(Y[mask][\"ConfirmedCases\"])-1, label=\"Real Data\",s=sz)\n",
    "ax1.scatter(x_[N:],predicted_confirmed_cases, label=\"Predicted\",s=sz)\n",
    "ax1.scatter(x_[N-num_validation_days:N],validation_confirmed_cases,label=\"Validation\",s=sz)\n",
    "if(PLOT_LINE):\n",
    "    ax1.plot(x_[:N],np.exp(Y[mask][\"ConfirmedCases\"])-1, label=\"Real Data\")\n",
    "    ax1.plot(x_[N:],predicted_confirmed_cases, label=\"Predicted\")\n",
    "    ax1.plot(x_[N-num_validation_days:N],validation_confirmed_cases,label=\"Validation\")\n",
    "ax1.set_title(region+\"Confirmed Cases\")\n",
    "ax1.legend()\n",
    "\n",
    "ax2.scatter(x_[:N],np.exp(Y[mask][\"Fatalities\"])-1, label=\"Real Data\",s=sz)\n",
    "ax2.scatter(x_[N:],predicted_fatalities, label=\"Predicted\",s=sz)\n",
    "ax2.scatter(x_[N-num_validation_days:N],validation_fatalities,label=\"Validation\",s=sz)\n",
    "if(PLOT_LINE):\n",
    "    ax2.plot(x_[:N],np.exp(Y[mask][\"Fatalities\"])-1, label=\"Real Data\")\n",
    "    ax2.plot(x_[N:],predicted_fatalities, label=\"Predicted\")\n",
    "    ax2.plot(x_[N-num_validation_days:N],validation_fatalities,label=\"Validation\")\n",
    "\n",
    "ax2.set_title(region+\"Fatalities\")\n",
    "ax2.legend()"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
